{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameter Tuning using HyperDrive\r\n",
    "\r\n",
    "Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "\r\n",
    "import azureml.core\r\n",
    "from azureml.core import Workspace\r\n",
    "from azureml.core import Experiment\r\n",
    "from azureml.core import Environment\r\n",
    "from azureml.core import ScriptRunConfig\r\n",
    "\r\n",
    "from azureml.core import Dataset"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1598423888013
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(azureml.core.VERSION)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\r\n",
    "\r\n",
    "Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "ws = Workspace.from_config()\r\n",
    "ws.write_config(\".azureml\")\r\n",
    "\r\n",
    "experiment_name = 'udacity-capstone-project'\r\n",
    "\r\n",
    "experiments = Experiment.list(ws, experiment_name=experiment_name)\r\n",
    "if not experiments:\r\n",
    "      experiment = Experiment(workspace=ws, name=experiment_name)\r\n",
    "else:\r\n",
    "      experiment = experiments[0]\r\n",
    "\r\n",
    "\r\n",
    "print('Workspace name: ' + ws.name, \r\n",
    "      'Azure region: ' + ws.location, \r\n",
    "      'Subscription id: ' + ws.subscription_id, \r\n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\r\n",
    "\r\n",
    "run = experiment.start_logging()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
    "from azureml.core.compute_target import ComputeTargetException\r\n",
    "\r\n",
    "# Create compute cluster or reuse it if it is available.\r\n",
    "cpu_cluster_name = \"cpu-cluster-01vx\"\r\n",
    "vm_size = \"Standard_D3_V2\"\r\n",
    "min_nodes = 0\r\n",
    "max_nodes = 6\r\n",
    "\r\n",
    "try:\r\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\r\n",
    "    print(\"Found existing cluster. use it\")\r\n",
    "except ComputeTargetException:\r\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\r\n",
    "                                                           min_nodes=min_nodes,\r\n",
    "                                                           max_nodes=max_nodes)\r\n",
    "    compute_target = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\r\n",
    "\r\n",
    "compute_target.wait_for_completion(show_output=True)\r\n",
    "\r\n",
    "# a detailed status for the current cluster.\r\n",
    "print(compute_target.get_status().serialize())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperdrive Configuration\r\n",
    "\r\n",
    "TODO: Explain the model you are using and the reason for chosing the different hyperparameters, termination policy and config settings.  \r\n",
    "Logistic regression was used to build the classification model. \r\n",
    "Two hyperparameters of the inverse of regularization (C) and the maximum number of iterations (max-iter) were chosen to run HyperDrive.\r\n",
    "C is chosen to control the effective size of parameters.\r\n",
    "max-iter was tested to determine how long the training should be done to find an optimal model.\r\n",
    "For each run, the two parameters were randomly selected (RandomParameter Sampling).\r\n",
    "For the early stopping policy, we chose BanditPolicy that seems to be efficient of choosing the early stopping point."
   ],
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598531923519
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import shutil\r\n",
    "\r\n",
    "from azureml.widgets import RunDetails\r\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\r\n",
    "from azureml.train.hyperdrive.policy import BanditPolicy\r\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\r\n",
    "from azureml.train.hyperdrive.parameter_expressions import uniform, choice\r\n",
    "\r\n",
    "from azureml.core import ScriptRunConfig\r\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create an early termination policy. This is not required if you are using Bayesian sampling.\r\n",
    "early_termination_policy = BanditPolicy(slack_factor=0.2, evaluation_interval=10, delay_evaluation=5) \r\n",
    "\r\n",
    "# Create the different params that you will be using during training\r\n",
    "parameter_space = { \"--C\": uniform(0.1, 1.0), \r\n",
    "                    \"--max_iter\": choice([50, 100, 500, 1000]) }\r\n",
    "param_sampling = RandomParameterSampling(parameter_space)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1598544893076
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "script_folder = \"training\"\r\n",
    "os.makedirs(os.path.join(\".\", script_folder), exist_ok=True)\r\n",
    "\r\n",
    "# Create a SKLearn estimator for use with train.py\r\n",
    "shutil.copy(\"./train.py\", os.path.join(\".\", script_folder))\r\n",
    "source_directory=os.path.join(\".\", script_folder)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%writefile conda_dependencies.yml\r\n",
    "\r\n",
    "dependencies:\r\n",
    "- python=3.6.2\r\n",
    "- numpy\r\n",
    "- pandas\r\n",
    "- scikit-learn\r\n",
    "- pip:\r\n",
    "  - azureml-defaults"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sklearn_env = Environment.from_conda_specification(name = 'sklearn-env', file_path = './conda_dependencies.yml')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Use ScrptRunConfig\r\n",
    "src = ScriptRunConfig(source_directory=source_directory,\r\n",
    "                      script='train.py',\r\n",
    "                      arguments=[],\r\n",
    "                      compute_target=compute_target,\r\n",
    "                      environment=sklearn_env)\r\n",
    "\r\n",
    "# Create a HyperDriveConfig\r\n",
    "hyperdrive_run_config = HyperDriveConfig(\r\n",
    "                        run_config=src,\r\n",
    "                        hyperparameter_sampling=param_sampling,\r\n",
    "                        policy=early_termination_policy,\r\n",
    "                        primary_metric_name=\"accuracy\",\r\n",
    "                        primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\r\n",
    "                        max_total_runs=30,\r\n",
    "                        max_concurrent_runs=max_nodes-1\r\n",
    "                        ) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Submit your experiment\r\n",
    "hyperdrive_run = experiment.submit(hyperdrive_run_config)"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1598544897941
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Details\r\n",
    "\r\n",
    "In the cell below, use the `RunDetails` widget to show the different experiments."
   ],
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598544898497
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "RunDetails(hyperdrive_run).show()"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1598546648408
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hyperdrive_run.wait_for_completion(show_output=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Best Model\r\n",
    "\r\n",
    "In the cell below, get the best model from the hyperdrive experiments and display all the properties of the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import joblib\r\n",
    "# Get your best run and save the model from that run.\r\n",
    "\r\n",
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\r\n",
    "best_run_metrics = best_run.get_metrics()\r\n",
    "parameter_values = best_run.get_details()[\"runDefinition\"][\"arguments\"]"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1598546650307
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Best Run Id      :\", best_run.id)\r\n",
    "print(\"Accuracy         :\", best_run_metrics[\"accuracy\"])\r\n",
    "print(\"--C              :\", parameter_values[1])\r\n",
    "print(\"--max-iter       :\", parameter_values[3])\r\n",
    "\r\n",
    "print(parameter_values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "best_run_filenames = best_run.get_file_names()\r\n",
    "for filename in best_run_filenames:\r\n",
    "    print(filename)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save the best model\r\n",
    "# The last element of best_run_filenames is a joblib file.\r\n",
    "best_run.download_file(best_run_filenames[-1], output_file_path=os.path.join(\".\", script_folder))"
   ],
   "outputs": [],
   "metadata": {
    "gather": {
     "logged": 1598546657829
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   }
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('azure': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "interpreter": {
   "hash": "b19a256a7d3812a8e0a3a61d7add31005087a7b63979c46bc7a1e05f0b16a33e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}